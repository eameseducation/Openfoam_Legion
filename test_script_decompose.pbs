#!/bin/bash -l
# Batch script to run an MPI parallel job with the upgraded software
# stack under SGE with Intel MPI.
# 1. Force bash as the executing shell.
#$ -S /bin/bash
# 2. Request ten minutes of wallclock time (format hours:minutes:seconds).
#$ -l h_rt=5:00:0
# 3. Request 1 gigabyte of RAM per process.
#$ -l mem=4G
# 4. Request 15 gigabyte of TMPDIR space per node (default is 10 GB)
#$ -l tmpfs=15G
# 5. Set the name of the job.
#$ -N M1.2Mesh
# 6. Select the MPI parallel environment and 64 processes.
#$ -pe mpi 1
# 7. Set the working directory to somewhere in your scratch space.  This is
# a necessary step with the upgraded software stack as compute nodes cannot
# write to $HOME.
# Replace "<your_UCL_id>" with your UCL user ID :
#$ -wd /home/ucemiea/Scratch/compressible_flow_cylinder/mesh/
# 8. Run our MPI job.  GERun is a wrapper that launches MPI jobs on our clusters.

#$ -ac allow=TU
 
module load python/2.7.9
module load boost/1_54_0/mpi/intel-2015-update2
module load openfoam/2.3.1/intel-2015-update2
module load mesa
module load gmsh

gmsh refined_mesh/cylinder_refined.geo -3 -o cylinder_refined.msh 
cp cylinder_refined.msh ~/Scratch/compressible_flow_cylinder/M1.2/
cd ../M1.2
gmshToFoam cylinder_refined.msh
checkMesh
decomposePar
